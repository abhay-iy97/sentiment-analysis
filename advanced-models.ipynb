{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief overview of approach for NLP HW2\n",
    "\n",
    "**Part 1**\n",
    "\n",
    "- We import the dataset and perform data cleaning as mentioned below.\n",
    "- From the original dataset we keep only the 'star_rating' and 'review_body' columns to train/test multiple classifiers.\n",
    "- We convert 'star_rating' to integer and 'review_body' to string for uniformity of datatypes across these columns.\n",
    "- We sample 20,000 random reviews from each 'star_rating' class.\n",
    "- Following data cleaning techniques have been performed on the dataset -\n",
    "    1. convert all reviews to lowercase\n",
    "    2. remove html tags and urls from reviews using BeautifulSoup\n",
    "    3. remove extra spaces in the reviews\n",
    "    4. remove punctuations\n",
    "    5. remove non-alphabetical characters\n",
    "- No data preprocessing techniques (like stopword removal/lemmatization) have been performed on the dataset.\n",
    "\n",
    "**Part 2**\n",
    "\n",
    "- First, we load a pretrained Word2Vec model and check for semantic similarities of various word combinations by generating their vectors. It was interesting to observe the results for a few cases. For instance, word embeddings of 'excellent' and 'outstanding' had a relatively low cosine similarity score. Additionally the combination 'germany' - 'berlin' + 'paris' had a low similarity score for 'france' and on using the most_similar() API, we notice that 'spain' has a higher cosine similarity than 'france' even though germany-berlin is a country-capital relationship so we would expect France to have the highest cosine similarity since France-paris is another country-capital relationship.\n",
    "- Second, we train our own Word2Vec model on the Amazon reviews dataset and perform the same semantic similarity experiments to observe differences. More insights have been provided in the respective section.\n",
    "\n",
    "**Part 3**\n",
    "\n",
    "- In this part, we used the word embeddings generated by the pretrained Word2Vec model to train simple models like Perceptron and SVM. It was interesting to see the difference in results between these classifiers and classifiers trained on TF-IDF features. More insights have been provided in the respective section.\n",
    "\n",
    "**Part 4**\n",
    "\n",
    "- Here, we train two models which are feedforward neural networks but with different training data. \n",
    "- The first model (a) - we utilize the input features generated in Part 3 to see how our model performs. We create an 80-20 split of training data like all other models and create their respective dataloaders and training loop.\n",
    "- For part (b), we generate input features by concatenating the first 10 Word2Vec vectors for each review. More insights for both models are provided in their respective sections.\n",
    "\n",
    "**Part 5**\n",
    "\n",
    "- Here, we train two models with different architectures (however both are recurrent neural networks) with the same training data and compare their performances.\n",
    "- For the model in part (a), we create a vanilla RNN with a hidden state size of 20 and pass it reviews which have a maximum length of 20 words. Reviews shorter than 20 words have been padded with tokens, while longer reviews have been truncated to 20 words.\n",
    "- In part (b), we create a model which utilizes GRU instead of a vanilla RNN cell.\n",
    "- More insights for both models are provided in their respective sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn imports\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Gensim imports\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "from gensim import utils\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = '/home1/adiyer/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz'\n",
    "try:\n",
    "    w2v_model = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "except:\n",
    "    w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Dataset generation\n",
    "\n",
    "**Note**\n",
    "- 80-20 split of dataset is performed below in multiple sections of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './amazon_reviews_us_Jewelry_v1_00.tsv'\n",
    "reviews_df = pd.read_csv(filepath, sep='\\t', on_bad_lines='skip', dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1766992 entries, 0 to 1766991\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Dtype \n",
      "---  ------             ----- \n",
      " 0   marketplace        object\n",
      " 1   customer_id        object\n",
      " 2   review_id          object\n",
      " 3   product_id         object\n",
      " 4   product_parent     object\n",
      " 5   product_title      object\n",
      " 6   product_category   object\n",
      " 7   star_rating        object\n",
      " 8   helpful_votes      object\n",
      " 9   total_votes        object\n",
      " 10  vine               object\n",
      " 11  verified_purchase  object\n",
      " 12  review_headline    object\n",
      " 13  review_body        object\n",
      " 14  review_date        object\n",
      "dtypes: object(15)\n",
      "memory usage: 202.2+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Keep Reviews and Ratings\n",
    "- Selecting only 'star_rating' and 'review_body'\n",
    "- We use 'review_body' to develop the input features\n",
    "- We use 'star_rating' as the target results which must be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df[['star_rating','review_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1766992 entries, 0 to 1766991\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   star_rating  object\n",
      " 1   review_body  object\n",
      "dtypes: object(2)\n",
      "memory usage: 27.0+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Randomly selecting reviews from each star_rating_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all 'star_rating' to integer representations\n",
    "# Select all rows which have 'star_rating' and 'review_body' as existing int/string values for optimal training results of the models\n",
    "\n",
    "reviews_df['star_rating'] = pd.to_numeric(reviews_df['star_rating'],errors='coerce')\n",
    "reviews_df = reviews_df[reviews_df['star_rating'].notna()]\n",
    "reviews_df = reviews_df[reviews_df['review_body'].notna()]\n",
    "\n",
    "#Convert all 'star_rating' to int\n",
    "reviews_df['star_rating'] = reviews_df['star_rating'].astype(int)\n",
    "\n",
    "#Convert all reviews to string\n",
    "reviews_df['review_body'] = reviews_df['review_body'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1766748 entries, 0 to 1766991\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   star_rating  int64 \n",
      " 1   review_body  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 40.4+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sampling 20,000 samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_1 = reviews_df[reviews_df.star_rating.eq(1)].sample(20000, random_state=1)\n",
    "rating_2 = reviews_df[reviews_df.star_rating.eq(2)].sample(20000, random_state=1)\n",
    "rating_3 = reviews_df[reviews_df.star_rating.eq(3)].sample(20000, random_state=1)\n",
    "rating_4 = reviews_df[reviews_df.star_rating.eq(4)].sample(20000, random_state=1)\n",
    "rating_5 = reviews_df[reviews_df.star_rating.eq(5)].sample(20000, random_state=1)\n",
    "\n",
    "sampled_reviews_df_20000 = pd.concat([rating_1, rating_2, rating_3, rating_4, rating_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100000 entries, 344647 to 1478372\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   star_rating  100000 non-null  int64 \n",
      " 1   review_body  100000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "sampled_reviews_df_20000.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Word Embedding\n",
    "#### Answers\n",
    "**1. What do you conclude from comparing vectors generated by yourself and the pretrained model?**\n",
    "The cosine similarity metric yields better results (i.e similar vectors have a higher cosine similarity score) for the pretrained model in comparison to the model trained by me.  This could be attributed to multiple reasons. Firstly, the Google model has been trained on 3 billion words extracted from a news dataset collated by Google wherein the pretrained model's vocabulary size is about 3 million words. This would result in a model which is extremely efficient in understanding semantic similarities in most contexts (especially news). However the Word2Vec model trained on the Amazon reviews dataset has a vocabulary in the range of thousands which is miniscule in comparison to the pretrained model wherein it was trained on 100,000 reviews. Secondly, our Word2Vec model has been limited to an embedding size of 300 and window size of 11 with a minimum word count of 10. This limitation might impede the the performance of the model since hyperparameter tuning is restricted.\n",
    "\n",
    "\n",
    "**2. Which of the Word2Vec models seems to encode semantic similarities between words better?**\n",
    "The pretrained 'word2vec-google-news-300' encodes semantic similarities better than the Word2Vec model which is trained on the Amazon reviews dataset. Overall, we could state that the pretrained Word2Vec model learned more meaningful relations between words in the corpus and encoded better semantic relationships of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Word2Vec model semantic similarity examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: [[0.73005176]]\n"
     ]
    }
   ],
   "source": [
    "value1 = (w2v_model['king'] - w2v_model['man'] + w2v_model['woman']).reshape(1, -1)\n",
    "value2 = (w2v_model['queen']).reshape(1, -1)\n",
    "print(f'Cosine similarity: {cosine_similarity(value1, value2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: [[0.5567486]]\n"
     ]
    }
   ],
   "source": [
    "value1 = (w2v_model['excellent']).reshape(1, -1)\n",
    "value2 = (w2v_model['outstanding']).reshape(1, -1)\n",
    "print(f'Cosine similarity: {cosine_similarity(value1, value2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Other examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: [[0.8671473]]\n"
     ]
    }
   ],
   "source": [
    "value1 = (w2v_model['father'] - w2v_model['man'] + w2v_model['woman']).reshape(1, -1)\n",
    "value2 = (w2v_model['mother']).reshape(1, -1)\n",
    "print(f'Cosine similarity: {cosine_similarity(value1, value2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: [[0.85979044]]\n"
     ]
    }
   ],
   "source": [
    "value1 = (w2v_model['man'] - w2v_model['woman'] + w2v_model['daughter']).reshape(1, -1)\n",
    "value2 = (w2v_model['son']).reshape(1, -1)\n",
    "print(f'Cosine similarity: {cosine_similarity(value1, value2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: [[0.7409728]]\n"
     ]
    }
   ],
   "source": [
    "value1 = (w2v_model['excellent']).reshape(1, -1)\n",
    "value2 = (w2v_model['terrific']).reshape(1, -1)\n",
    "print(f'Cosine similarity: {cosine_similarity(value1, value2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: [[0.54980403]]\n"
     ]
    }
   ],
   "source": [
    "value1 = (w2v_model['germany'] - w2v_model['berlin'] + w2v_model['paris']).reshape(1, -1)\n",
    "value2 = (w2v_model['france']).reshape(1, -1)\n",
    "print(f'Cosine similarity: {cosine_similarity(value1, value2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Checking examples with most_similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrific', 0.7409726977348328),\n",
       " ('superb', 0.7062715888023376),\n",
       " ('exceptional', 0.681470513343811),\n",
       " ('fantastic', 0.6802847981452942),\n",
       " ('good', 0.6442928910255432),\n",
       " ('great', 0.6124600172042847),\n",
       " ('Excellent', 0.6091997623443604),\n",
       " ('impeccable', 0.5980967283248901),\n",
       " ('exemplary', 0.5959650278091431),\n",
       " ('marvelous', 0.582928478717804)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(positive=['excellent'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mother', 0.8462507128715515)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(positive=['woman', 'father'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('son', 0.8490632772445679)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(positive=['man', 'daughter'], negative=['woman'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spain', 0.5414217114448547),\n",
       " ('france', 0.5339585542678833),\n",
       " ('europe', 0.5268116593360901),\n",
       " ('british', 0.5118182897567749),\n",
       " ('wiv', 0.5049731731414795),\n",
       " ('india', 0.5046922564506531),\n",
       " ('portugal', 0.5025326609611511),\n",
       " ('england', 0.49969735741615295),\n",
       " ('russia', 0.498568058013916),\n",
       " ('thailand', 0.49663135409355164)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interesting to note that Spain is more similar than France\n",
    "w2v_model.most_similar(positive=['germany', 'paris'], negative=['berlin'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training word2vec model on Amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonReviewsCorpus:\n",
    "    \"\"\"An iterator that gives sentences as a list of strings.\"\"\"\n",
    "    def __iter__(self):\n",
    "        for line in sampled_reviews_df_20000['review_body'].to_list():\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = AmazonReviewsCorpus()\n",
    "model = Word2Vec(sentences=reviews, vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: [[0.2853059]]\n"
     ]
    }
   ],
   "source": [
    "value1 = (model.wv['king'] - model.wv['man'] + model.wv['woman']).reshape(1, -1)\n",
    "value2 = (model.wv['queen']).reshape(1, -1)\n",
    "print(f'Cosine similarity: {cosine_similarity(value1, value2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: [[0.8820023]]\n"
     ]
    }
   ],
   "source": [
    "value1 = (model.wv['excellent']).reshape(1, -1)\n",
    "value2 = (model.wv['outstanding']).reshape(1, -1)\n",
    "print(f'Cosine similarity: {cosine_similarity(value1, value2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Other examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: [[0.54931146]]\n"
     ]
    }
   ],
   "source": [
    "value1 = (model.wv['father'] - model.wv['man'] + model.wv['woman']).reshape(1, -1)\n",
    "value2 = (model.wv['mother']).reshape(1, -1)\n",
    "print(f'Cosine similarity: {cosine_similarity(value1, value2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Insights\n",
    "- The error below has been left on purpose in the notebook. This shows us how powerful/huge the pretrained model is in comparison to the Word2Vec model trained on the Amazon reviews dataset. For instance, Germany is not present in the vocabulary of the model trained in part (b) which means that none of the reviews referenced this particular word. This could be improved by training our Word2Vec model on a huger dataset which could enrich its vocabulary and word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'germany' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/SLURM_11124997/ipykernel_69937/1438926971.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalue1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'germany'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'berlin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paris'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalue2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'france'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Cosine similarity: {cosine_similarity(value1, value2)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/timesformer/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \"\"\"\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/timesformer/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \"\"\"\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/timesformer/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'germany' not present\""
     ]
    }
   ],
   "source": [
    "value1 = (model.wv['germany'] - model.wv['berlin'] + model.wv['paris']).reshape(1, -1)\n",
    "value2 = (model.wv['france']).reshape(1, -1)\n",
    "print(f'Cosine similarity: {cosine_similarity(value1, value2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Simple Models\n",
    "#### Answers\n",
    "\n",
    "**1. What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)**\n",
    "\n",
    "- For the perceptron model we observe\n",
    "\n",
    "    - Model trained on TFIDF Features - Overall accuracy - 50%\n",
    "    - Model trained on Word2Vec Features - Overall accuracy - 39%\n",
    "\n",
    "\n",
    "- For the SVM model we observe\n",
    "\n",
    "    - Model trained on TFIDF Features - Overall accuracy - 51%\n",
    "    - Model trained on Word2Vec Features - Overall accuracy - 49%\n",
    "\n",
    "This goes to show that there is no single data preprocessing step which could lead to best results for all models. For example, the perceptron model has a significant difference in overall accuracy when trained on word2vec vs tfidf which could mean that the tfidf extracted features were better understood by the single layer perceptron in comparison to the features provided by the Google word2vec model. However for SVM, both techniques have a marginal difference and could mean that for the SVM model both techniques work and could be used in different situations based on the requirement.\n",
    "\n",
    "Overall it looks like TFIDF features performed better for both models (When compared with only accuracy). Additionally, SVM with TFIDF features has the best accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data cleaning and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def URLRemoval(sentence):\n",
    "    \"\"\"Function to remove the HTML tags and URLs from reviews using BeautifulSoup\n",
    "\n",
    "    Args:\n",
    "        sentence (string): Sentence from which we remove the HTML tags and URLs\n",
    "\n",
    "    Returns:\n",
    "        string: Sentence which does not contain any HTML tags and URLs\n",
    "    \"\"\"\n",
    "    return BeautifulSoup(sentence, 'lxml').get_text() \n",
    "\n",
    "def nonAlphabeticRemoval(sentence):\n",
    "    \"\"\"Function to remove non-alphabetic characters from the sentence. \n",
    "    Note - We do not remove spaces from the sentence however extra spaces are removed in a different function\n",
    "\n",
    "    Args:\n",
    "        sentence (string): Sentence from which we remove the non-alphabetic characters\n",
    "\n",
    "    Returns:\n",
    "        string: Sentence from which non-alphabetic characters have been removed\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[^a-zA-Z ]+\", \"\", sentence)  #This will also remove numbers.\n",
    "\n",
    "def removeExtraSpaces(sentence):\n",
    "    \"\"\"Remove extra spaces from the sentence\n",
    "\n",
    "    Args:\n",
    "        sentence (string): Sentence from which we remove extra spaces\n",
    "\n",
    "    Returns:\n",
    "        string: Sentence from which extra spaces have been removed\n",
    "    \"\"\"\n",
    "    return ' '.join(sentence.split())\n",
    "\n",
    "def removePunctuation(sentence):\n",
    "    \"\"\"Function to remove punctuations from a sentence\n",
    "\n",
    "    Args:\n",
    "        sentence (string): Sentence from which punctuations have to be removed\n",
    "\n",
    "    Returns:\n",
    "        string: Sentence from which punctuations have been removed\n",
    "    \"\"\"\n",
    "    for value in string.punctuation:\n",
    "        if value in sentence:\n",
    "            sentence = sentence.replace(value, ' ')\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateW2VFeatures(word):\n",
    "    \"\"\"Function to return word2vec word embeddings for a particular word\n",
    "\n",
    "    Args:\n",
    "        word (string): Word for which we need to generate embeddings\n",
    "\n",
    "    Returns:\n",
    "        np array: Vector of word embeddings which is extracted from the pretrained word2vec model\n",
    "    \"\"\"\n",
    "    return w2v_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayReport(actualLabels, predictedLabels, classifierName):\n",
    "    \"\"\"Function to display precision/recall/f1-score metrics for a classifier\n",
    "\n",
    "    Args:\n",
    "        actualLabels (_type_): True labels of the data\n",
    "        predictedLabels (_type_): Labels predicted by the classifier\n",
    "        classifierName (_type_): Name of the classifier which predicted the labels\n",
    "    \"\"\"\n",
    "    targetNames = ['Rating 1', 'Rating 2', 'Rating 3', 'Rating 4', 'Rating 5']\n",
    "    report = classification_report(actualLabels, predictedLabels, target_names=targetNames, output_dict=True)\n",
    "    print(f'Accuracy: {report[\"accuracy\"]}')\n",
    "    print('============================================================')\n",
    "    print(f'Precision, Recall, f1-score for Testing split for {classifierName}')\n",
    "    print('============================================================')\n",
    "    for targetClass in targetNames:\n",
    "        print(f'{targetClass}: {report[targetClass][\"precision\"]}, {report[targetClass][\"recall\"]}, {report[targetClass][\"f1-score\"]}')\n",
    "    print(f'Macro Average: {report[\"macro avg\"][\"precision\"]}, {report[\"macro avg\"][\"recall\"]}, {report[\"macro avg\"][\"f1-score\"]}')\n",
    "    print('============================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Cleaning\n",
    "1. Convert all reviews into lower case\n",
    "2. Remove HTML and URLs from the reviews\n",
    "3. Remove punctuations\n",
    "4. Remove extra spaces and non-alphabetic characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Convert all reviews into the lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_df_20000['review_body'] = sampled_reviews_df_20000['review_body'].apply(lambda value:value.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/adiyer/.conda/envs/timesformer/lib/python3.7/site-packages/bs4/__init__.py:439: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    }
   ],
   "source": [
    "sampled_reviews_df_20000['review_body'] = sampled_reviews_df_20000['review_body'].apply(URLRemoval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_df_20000['review_body'] = sampled_reviews_df_20000['review_body'].apply(removeExtraSpaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_df_20000['review_body'] = sampled_reviews_df_20000['review_body'].apply(removePunctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove extra spaces - doing it again because removing punctuations creates extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_df_20000['review_body'] = sampled_reviews_df_20000['review_body'].apply(removeExtraSpaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove non-alphabetical characters (excluding spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_df_20000['review_body'] = sampled_reviews_df_20000['review_body'].apply(nonAlphabeticRemoval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>344647</th>\n",
       "      <td>1</td>\n",
       "      <td>bought two pairs one set for each twin one pai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903655</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the earring but the back didn t stay o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600617</th>\n",
       "      <td>1</td>\n",
       "      <td>smaller than i expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075405</th>\n",
       "      <td>1</td>\n",
       "      <td>when i saw the price for the ring pictured i j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001976</th>\n",
       "      <td>1</td>\n",
       "      <td>piece of crap i cannot believe how bad this ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281719</th>\n",
       "      <td>1</td>\n",
       "      <td>the balls wouldn t stay in the g already lost ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572384</th>\n",
       "      <td>1</td>\n",
       "      <td>it looks cheap and the eyes of hello kitty lef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499950</th>\n",
       "      <td>1</td>\n",
       "      <td>they broke the first day i wore them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283936</th>\n",
       "      <td>1</td>\n",
       "      <td>so i purchased this item thinking it d be a go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322181</th>\n",
       "      <td>1</td>\n",
       "      <td>tarnished after a few months would not recommend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "344647             1  bought two pairs one set for each twin one pai...\n",
       "903655             1  i liked the earring but the back didn t stay o...\n",
       "600617             1                            smaller than i expected\n",
       "1075405            1  when i saw the price for the ring pictured i j...\n",
       "1001976            1  piece of crap i cannot believe how bad this ri...\n",
       "281719             1  the balls wouldn t stay in the g already lost ...\n",
       "1572384            1  it looks cheap and the eyes of hello kitty lef...\n",
       "499950             1               they broke the first day i wore them\n",
       "1283936            1  so i purchased this item thinking it d be a go...\n",
       "322181             1   tarnished after a few months would not recommend"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_reviews_df_20000.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate W2V Features\n",
    "- Compute average Word2Vec vectors for each review as the input feature by averaging the vector embeddings of the reviews\n",
    "- If the word does not exist in the pretrained Word2Vec model's vocabulary - we add a vector filled with 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:08<00:00, 11619.94it/s]\n"
     ]
    }
   ],
   "source": [
    "unknownKeyValue, finalW2VFeatures = np.zeros((300,)), []\n",
    "\n",
    "finalW2VFeatures = [np.mean([generateW2VFeatures(word) if word in w2v_model else unknownKeyValue for word in sentence.split(' ') ],axis=0) for sentence in tqdm(sampled_reviews_df_20000['review_body'].tolist())]\n",
    "arrayW2VFeatures = np.array(finalW2VFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 300)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrayW2VFeatures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 80-20 train-test split is created in the next section with stratification being performed for equal representation of classes in the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, testData, trainLabels, testLabels = train_test_split(arrayW2VFeatures, sampled_reviews_df_20000['star_rating'].to_list(), test_size=0.2, random_state=42, stratify=sampled_reviews_df_20000['star_rating'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 20000\n"
     ]
    }
   ],
   "source": [
    "print(len(trainData), len(testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptronModel = Perceptron(eta0=0.1, tol=1e-5, n_jobs=-1, max_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(eta0=0.1, max_iter=5000, n_jobs=-1, tol=1e-05)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptronModel.fit(trainData, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.38985\n",
      "============================================================\n",
      "Precision, Recall, f1-score for Testing split for Perceptron\n",
      "============================================================\n",
      "Rating 1: 0.4558487113008998, 0.74725, 0.5662593539831392\n",
      "Rating 2: 0.3593314763231198, 0.16125, 0.22260569456427956\n",
      "Rating 3: 0.37103448275862067, 0.269, 0.3118840579710145\n",
      "Rating 4: 0.33056158110388045, 0.68575, 0.4460887949260042\n",
      "Rating 5: 0.7644444444444445, 0.086, 0.1546067415730337\n",
      "Macro Average: 0.4562441391861931, 0.38985, 0.3402889286034942\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "predictedLabels = perceptronModel.predict(testData)\n",
    "displayReport(testLabels, predictedLabels, 'Perceptron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmModel = LinearSVC().fit(trainData, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48695\n",
      "============================================================\n",
      "Precision, Recall, f1-score for Testing split for SVM\n",
      "============================================================\n",
      "Rating 1: 0.5092035093755376, 0.74, 0.6032813614592887\n",
      "Rating 2: 0.40763052208835343, 0.25375, 0.31278890600924497\n",
      "Rating 3: 0.4087530966143683, 0.37125, 0.3890999606969737\n",
      "Rating 4: 0.4299965600275198, 0.3125, 0.3619516432604604\n",
      "Rating 5: 0.5873569904983518, 0.75725, 0.6615703833133122\n",
      "Macro Average: 0.46858813572082614, 0.48695000000000005, 0.46573845094785593\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "predictedLabels = svmModel.predict(testData)\n",
    "displayReport(testLabels, predictedLabels, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feedforward Neural Networks\n",
    "### Answers\n",
    "\n",
    "**1. What do you conclude by comparing accuracy values you obtain with those obtained in the \"Simple Models\" section?**\n",
    "\n",
    "- From the accuracies achieved by the Feedforward neural networks we observe that there is not a huge difference/significant improvement in performance compared to the simple models. For instance, the SVM with TFIDF features outperforms all the models (simple and feedforward models). Limiting the scope of models to HW2 - we observe that the Feedforward neural network in 4A achieves the highest accuracy value of 0.5066 which marginally better when compared to the SVM model trained on Word2Vec features. This leads us to the conclusion that statistical models like the SVM/Single layer perceptron can still be leveraged for such scenarios and it is interesting to observe that more powerful models do not always lead to the best performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset creation\n",
    "- Creating a custom dataset class for Amazon reviews to enable the creation of the relevant dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAmazonDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We modify the train/test labels of this dataset since we utilize the cross-entropy loss function. Class labels must range from [0, C) where C is the number of classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification required for cross-entropy loss.\n",
    "def modifyLabels(labels):\n",
    "    \"\"\"Function to modify labels and reduce them by 1 due to the requirements of cross-entropy loss for target variables\n",
    "\n",
    "    Args:\n",
    "        labels (list): List of labels\n",
    "\n",
    "    Returns:\n",
    "        list: List of labels with every element reduced by 1 since our ratings are in the range of [1,5]. We return a list with values in range of [0,4]\n",
    "    \"\"\"\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] -= 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = modifyLabels(trainLabels)\n",
    "testLabels = modifyLabels(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = CustomAmazonDataset(trainData, trainLabels, transform=transforms.ToTensor())\n",
    "dataTest = CustomAmazonDataset(testData, testLabels, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "batch_size = 512\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = DataLoader(dataTrain, batch_size=batch_size, shuffle=True)\n",
    "testLoader = DataLoader(dataTest, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part - 4a\n",
    "\n",
    "- Accuracy achieved - 0.5066"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, inputDim):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        hiddenLayer1 = 50\n",
    "        hiddenLayer2 = 10\n",
    "        self.fc1 = nn.Linear(inputDim, hiddenLayer1)\n",
    "        self.fc2 = nn.Linear(hiddenLayer1, hiddenLayer2)\n",
    "        self.fc3 = nn.Linear(hiddenLayer2, 5)\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.002582\n",
      "Epoch: 2 \tTraining Loss: 0.002321\n",
      "Epoch: 3 \tTraining Loss: 0.002277\n",
      "Epoch: 4 \tTraining Loss: 0.002258\n",
      "Epoch: 5 \tTraining Loss: 0.002235\n",
      "Epoch: 6 \tTraining Loss: 0.002225\n",
      "Epoch: 7 \tTraining Loss: 0.002212\n",
      "Epoch: 8 \tTraining Loss: 0.002195\n",
      "Epoch: 9 \tTraining Loss: 0.002189\n",
      "Epoch: 10 \tTraining Loss: 0.002184\n",
      "Epoch: 11 \tTraining Loss: 0.002179\n",
      "Epoch: 12 \tTraining Loss: 0.002163\n",
      "Epoch: 13 \tTraining Loss: 0.002157\n",
      "Epoch: 14 \tTraining Loss: 0.002156\n",
      "Epoch: 15 \tTraining Loss: 0.002148\n",
      "Epoch: 16 \tTraining Loss: 0.002141\n",
      "Epoch: 17 \tTraining Loss: 0.002137\n",
      "Epoch: 18 \tTraining Loss: 0.002137\n",
      "Epoch: 19 \tTraining Loss: 0.002128\n",
      "Epoch: 20 \tTraining Loss: 0.002119\n",
      "Epoch: 21 \tTraining Loss: 0.002121\n",
      "Epoch: 22 \tTraining Loss: 0.002117\n",
      "Epoch: 23 \tTraining Loss: 0.002112\n",
      "Epoch: 24 \tTraining Loss: 0.002108\n",
      "Epoch: 25 \tTraining Loss: 0.002107\n",
      "Epoch: 26 \tTraining Loss: 0.002104\n",
      "Epoch: 27 \tTraining Loss: 0.002095\n",
      "Epoch: 28 \tTraining Loss: 0.002091\n",
      "Epoch: 29 \tTraining Loss: 0.002088\n",
      "Epoch: 30 \tTraining Loss: 0.002094\n"
     ]
    }
   ],
   "source": [
    "######### MODEL CREATION ###########\n",
    "inputDim = 300\n",
    "model = MultiLayerPerceptron(inputDim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "######### MODEL TRAINING ###########\n",
    "for epoch in range(epochs):\n",
    "    trainLoss = 0.0\n",
    "    validationLoss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for data, label in trainLoader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.float())\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainLoss += loss.item()\n",
    "    trainLoss = trainLoss/len(trainLoader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, trainLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, modelName, device):\n",
    "    \"\"\"Function to utilize the trained model and predict on the samples in the dataloader. Also prints the accuracy of the model.\n",
    "\n",
    "    Args:\n",
    "        model (MultiLayerPerceptron): Trained MLP model\n",
    "        dataloader (Dataloader): Word for which we need to generate embeddings\n",
    "        modelName (string): Name of model to differentiate between different MLP models (For question 4a and 4b)\n",
    "        device (string): 'cuda' or 'cpu' based on GPU availability\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs.float())\n",
    "        value, predictions = torch.max(outputs.data, 1)\n",
    "        correct += (predictions == targets).sum().item()\n",
    "    print(f'Accuracy of {modelName} - {correct/len(dataloader.dataset)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of FFNN - Part A - 0.5066\n"
     ]
    }
   ],
   "source": [
    "predict(model, testLoader, 'FFNN - Part A', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Part 4b\n",
    "- Accuracy achieved - 0.3945"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Generate new W2V Features\n",
    "- Iterate over the reviews and concatenate the first 10 word embeddings. If the review is shorter than 10 words, we add padding in the form of an unknownKeyValue which is an np array of 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:07<00:00, 12940.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100000, 3000)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate first 10 Word2Vec vectors for each review and train the neural network\n",
    "unknownKeyValue, newFinalW2VFeatures = np.zeros((300,)), []\n",
    "\n",
    "for sentence in tqdm(sampled_reviews_df_20000['review_body'].tolist()):\n",
    "    sentenceList, outputArr = sentence.split(' '), []\n",
    "\n",
    "    for word in sentenceList:\n",
    "        if word in w2v_model:\n",
    "            outputArr.append(generateW2VFeatures(word))\n",
    "        else:\n",
    "            outputArr.append(unknownKeyValue)\n",
    "\n",
    "    if len(sentenceList) < 10:\n",
    "        for i in range(10 - len(sentenceList)):\n",
    "            outputArr.append(unknownKeyValue)\n",
    "    newFinalW2VFeatures.append(np.array(outputArr[:10]).reshape(3000,))\n",
    "\n",
    "newArrayW2VFeatures = np.array(newFinalW2VFeatures)\n",
    "newArrayW2VFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTrainData, newTestData, newTrainLabels, newTestLabels = train_test_split(newArrayW2VFeatures, sampled_reviews_df_20000['star_rating'].to_list(), test_size=0.2, random_state=42, stratify=sampled_reviews_df_20000['star_rating'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTrainLabels = modifyLabels(newTrainLabels)\n",
    "newTestLabels = modifyLabels(newTestLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDataTrain = CustomAmazonDataset(newTrainData, newTrainLabels, transform=transforms.ToTensor())\n",
    "newDataTest = CustomAmazonDataset(newTestData, newTestLabels, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTrainLoader = DataLoader(newDataTrain, batch_size=batch_size, shuffle=True)\n",
    "newTestLoader = DataLoader(newDataTest, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.002847\n",
      "Epoch: 2 \tTraining Loss: 0.002579\n",
      "Epoch: 3 \tTraining Loss: 0.002509\n",
      "Epoch: 4 \tTraining Loss: 0.002458\n",
      "Epoch: 5 \tTraining Loss: 0.002406\n",
      "Epoch: 6 \tTraining Loss: 0.002357\n",
      "Epoch: 7 \tTraining Loss: 0.002309\n",
      "Epoch: 8 \tTraining Loss: 0.002259\n",
      "Epoch: 9 \tTraining Loss: 0.002214\n",
      "Epoch: 10 \tTraining Loss: 0.002169\n",
      "Epoch: 11 \tTraining Loss: 0.002123\n",
      "Epoch: 12 \tTraining Loss: 0.002082\n",
      "Epoch: 13 \tTraining Loss: 0.002044\n",
      "Epoch: 14 \tTraining Loss: 0.001999\n",
      "Epoch: 15 \tTraining Loss: 0.001962\n",
      "Epoch: 16 \tTraining Loss: 0.001926\n",
      "Epoch: 17 \tTraining Loss: 0.001886\n",
      "Epoch: 18 \tTraining Loss: 0.001854\n",
      "Epoch: 19 \tTraining Loss: 0.001816\n",
      "Epoch: 20 \tTraining Loss: 0.001782\n",
      "Epoch: 21 \tTraining Loss: 0.001749\n",
      "Epoch: 22 \tTraining Loss: 0.001724\n",
      "Epoch: 23 \tTraining Loss: 0.001690\n",
      "Epoch: 24 \tTraining Loss: 0.001652\n",
      "Epoch: 25 \tTraining Loss: 0.001630\n",
      "Epoch: 26 \tTraining Loss: 0.001594\n",
      "Epoch: 27 \tTraining Loss: 0.001567\n",
      "Epoch: 28 \tTraining Loss: 0.001544\n",
      "Epoch: 29 \tTraining Loss: 0.001515\n",
      "Epoch: 30 \tTraining Loss: 0.001489\n"
     ]
    }
   ],
   "source": [
    "######### MODEL CREATION ###########\n",
    "inputDim = 3000\n",
    "modelB = MultiLayerPerceptron(inputDim)\n",
    "criterionB = nn.CrossEntropyLoss()\n",
    "optimizerB = torch.optim.Adam(modelB.parameters(), lr=1e-3)\n",
    "deviceB = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "modelB = modelB.to(deviceB)\n",
    "\n",
    "\n",
    "######### MODEL TRAINING ###########\n",
    "for epoch in range(epochs):\n",
    "    trainLoss = 0.0    \n",
    "    modelB.train()\n",
    "    for data, label in newTrainLoader:\n",
    "        data, label = data.to(deviceB), label.to(deviceB)\n",
    "        optimizerB.zero_grad()\n",
    "        output = modelB(data.float())\n",
    "        loss = criterionB(output, label)\n",
    "        loss.backward()\n",
    "        optimizerB.step()\n",
    "        trainLoss += loss.item()\n",
    "    trainLoss = trainLoss/len(newTrainLoader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, trainLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of FFNN - Part B - 0.3945\n"
     ]
    }
   ],
   "source": [
    "predict(modelB, newTestLoader, 'FFNN - Part B', deviceB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "### Answers\n",
    "**1. What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models?**\n",
    "\n",
    "The accuracy value obtained by the simple RNN is 0.47445 which is better than the accuracy value obtained by the FFNN in 4b (0.3945) but less in comparison to the FFNN model trained in 4a (0.5066). This shows that even though we're utilizing RNNs which are supposedly better for the task of analyzing sequential data - our results indicate a mixed performance. This could be attributed to multiple reasons. Our simple RNN could be improved in architecture to outperform the FFNN models. For instance, we could enhance our model by utilizing regularization techniques like dropout which might show better generalization on the test dataset. Hyperparameters could be better tuned to improve the RNN/FFNN performances.\n",
    "\n",
    "**2. What do you conclude by comparing accuracy values you obtain with those obtained using simple RNN.**\n",
    "\n",
    "In 5b - our model is trained with a GRU layer instead of RNN. Here, we observe that the model with GRU performs marginally better (0.49195) in comparison to the model with RNN (0.47445). This could potentially be due to the structure of the GRU wherein the gates determines the amount of previous sequential information that must pass to the next state. This also helps reduce the chances of experiencing vanishing gradients which Vanilla RNNs are extremely prone to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 5a\n",
    "- Accuracy achieved - 0.47445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecNeuralNet(nn.Module):\n",
    "    def __init__(self, vocabSize, embeddingDim, hiddenDim, embeddings, outputSize):\n",
    "        super(RecNeuralNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocabSize, embeddingDim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        self.rnn = nn.RNN(embeddingDim, hiddenDim, batch_first=True, nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hiddenDim, outputSize)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.embeddings(input)\n",
    "        rnnOutput, hidden = self.rnn(output)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate W2V Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Counting the occurrences of words in the Amazon reviews corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for idx, rowValue in sampled_reviews_df_20000.iterrows():\n",
    "    counter.update(rowValue['review_body'].split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining an embedding matrix\n",
    "- We create a function to return the pretrained word embeddings for words which occur in the amazon reviews corpus.\n",
    "- Additionally, we also return the vocabulary of the corpus i.e all the words which occur in the corpus.\n",
    "- Moreover, we generate indices for every word in the corpus and add two more word embeddings. \"\" - for padding with a word embedding equal to 0 and \"unknown\" for words which do not have word embeddings in the pretrained Word2Vec model. For words which are associated with the 'unknown' tag - we generate a word embedding by drawing samples from a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeEmbeddingMatrix(originalWordEmbeddings, words, embeddingDim = 300):\n",
    "    \"\"\"Function to compute the embedding matrix for the Amazon reviews dataset. This embedding matrix is used in the RNN.\n",
    "\n",
    "    Args:\n",
    "        originalWordEmbeddings (KeyedVector): Pretrained word2vec model\n",
    "        words (dictionary): Dictionary which stores all words which occur in the corpus and their counts\n",
    "        embeddingDim (int): Embedding dimension of the word vectors\n",
    "\n",
    "    Returns:\n",
    "        np.array: Stores the word embeddings at indices which match the indices mapped in the dictionary returned to the user\n",
    "        np.array: Array which contains all the words which occur in the corpus\n",
    "        dict: Dictionary which maps all words in the vocabulary to indices\n",
    "        \n",
    "    \"\"\"\n",
    "    vocabSize, vocabIndicesDict, vocab = len(words) + 2, {}, ['', 'unknown']\n",
    "    index = 2 # We initialize the first 2 elements of amazonWeightsMatrix below manually. All other elements are set in the for loop.\n",
    "    vocabIndicesDict[vocab[0]], vocabIndicesDict[vocab[1]] = 0, 1\n",
    "    amazonWeightsMatrix = np.zeros((vocabSize, embeddingDim), dtype=\"float32\")\n",
    "    \n",
    "    # amazonWeightsMatrix[0] - word embedding for padding token\n",
    "    # amazonWeightsMatrix[1] - word embedding for unknown token\n",
    "    \n",
    "    amazonWeightsMatrix[0], amazonWeightsMatrix[1]= np.zeros(embeddingDim, dtype='float32'), np.random.uniform(-0.25, 0.25, embeddingDim)\n",
    "    \n",
    "    for word in words:\n",
    "        amazonWeightsMatrix[index] = originalWordEmbeddings[word] if word in originalWordEmbeddings else np.random.uniform(-0.25,0.25, embeddingDim)\n",
    "        vocabIndicesDict[word] = index\n",
    "        vocab.append(word)\n",
    "        index += 1\n",
    "    return amazonWeightsMatrix, np.array(vocab), vocabIndicesDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iterate over the reviews and limit the maximum review length to 20. If the review is shorter than 20 words, we add padding which is mapped to an np array of 0s. If it is greater than 20 words, we truncate the review and only take the first 20 words into consideration\n",
    "- Here padding is 0 and unknown key value is 1. These are indices/locations of these tokens in the amazonWeightsMatrix\n",
    "- In the inner for loop we create an array(outputArr) of indices which is then appended to the outer array newFinalW2VFeatures.\n",
    "- Thus, we create a (100000,20) vector where every review has 20 tokens which are the indices of it's word embeddings in the amazonWeightsMatrix\n",
    "- We load the amazonWeightsMatrix as pretrained weights for the Embeddings layer in the RNN architecture and pass it the indices to help with the lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 77315.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100000, 20)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding, unknownKeyValue, newFinalW2VFeatures = 0, 1, [] \n",
    "amazonWeightsMatrix, vocab, vocabDict = computeEmbeddingMatrix(w2v_model, counter)\n",
    "\n",
    "for sentence in tqdm(sampled_reviews_df_20000['review_body'].tolist()):\n",
    "    sentenceList, outputArr = sentence.split(' '), []\n",
    "\n",
    "    for word in sentenceList:\n",
    "        if word in vocabDict:\n",
    "            outputArr.append(vocabDict[word])\n",
    "        else:\n",
    "            outputArr.append(unknownKeyValue)\n",
    "    if len(sentenceList) < 20:                        # Limit the max review length to 20\n",
    "        for i in range(20 - len(sentenceList)):\n",
    "            outputArr.append(padding)\n",
    "    newFinalW2VFeatures.append(np.array(outputArr[:20]))\n",
    "\n",
    "newFinalW2VFeatures = np.array(newFinalW2VFeatures)\n",
    "newFinalW2VFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, testData, trainLabels, testLabels = train_test_split(newFinalW2VFeatures, sampled_reviews_df_20000['star_rating'].to_list(), test_size=0.2, random_state=42, stratify=sampled_reviews_df_20000['star_rating'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = modifyLabels(trainLabels)\n",
    "testLabels = modifyLabels(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = CustomAmazonDataset(trainData, trainLabels, transform=transforms.ToTensor())\n",
    "dataTest = CustomAmazonDataset(testData, testLabels, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "batchSize = 512\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = DataLoader(dataTrain, batch_size=batchSize, shuffle=True)\n",
    "testLoader = DataLoader(dataTest, batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.003137\n",
      "Epoch: 2 \tTraining Loss: 0.002718\n",
      "Epoch: 3 \tTraining Loss: 0.002579\n",
      "Epoch: 4 \tTraining Loss: 0.002528\n",
      "Epoch: 5 \tTraining Loss: 0.002494\n",
      "Epoch: 6 \tTraining Loss: 0.002461\n",
      "Epoch: 7 \tTraining Loss: 0.002431\n",
      "Epoch: 8 \tTraining Loss: 0.002404\n",
      "Epoch: 9 \tTraining Loss: 0.002383\n",
      "Epoch: 10 \tTraining Loss: 0.002366\n",
      "Epoch: 11 \tTraining Loss: 0.002354\n",
      "Epoch: 12 \tTraining Loss: 0.002343\n",
      "Epoch: 13 \tTraining Loss: 0.002332\n",
      "Epoch: 14 \tTraining Loss: 0.002326\n",
      "Epoch: 15 \tTraining Loss: 0.002318\n",
      "Epoch: 16 \tTraining Loss: 0.002313\n",
      "Epoch: 17 \tTraining Loss: 0.002306\n",
      "Epoch: 18 \tTraining Loss: 0.002302\n",
      "Epoch: 19 \tTraining Loss: 0.002293\n",
      "Epoch: 20 \tTraining Loss: 0.002291\n",
      "Epoch: 21 \tTraining Loss: 0.002284\n",
      "Epoch: 22 \tTraining Loss: 0.002282\n",
      "Epoch: 23 \tTraining Loss: 0.002278\n",
      "Epoch: 24 \tTraining Loss: 0.002282\n",
      "Epoch: 25 \tTraining Loss: 0.002274\n",
      "Epoch: 26 \tTraining Loss: 0.002268\n",
      "Epoch: 27 \tTraining Loss: 0.002262\n",
      "Epoch: 28 \tTraining Loss: 0.002264\n",
      "Epoch: 29 \tTraining Loss: 0.002260\n",
      "Epoch: 30 \tTraining Loss: 0.002257\n"
     ]
    }
   ],
   "source": [
    "############## MODEL CREATION ##############\n",
    "hiddenDim = 20\n",
    "outputDim = 5\n",
    "learningRate = 1e-3\n",
    "numEpochs = 30\n",
    "embeddingDim = 300\n",
    "\n",
    "model = RecNeuralNet(vocab.shape[0], embeddingDim, hiddenDim, amazonWeightsMatrix, outputDim)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "############## MODEL TRAINING ##############\n",
    "for epoch in range(numEpochs):\n",
    "    trainLoss = 0.0\n",
    "    for data, labels in trainLoader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainLoss += loss.item()\n",
    "    trainLoss = trainLoss/len(trainLoader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, trainLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRNN(model, dataloader, modelName, device):\n",
    "    \"\"\"Function to utilize the trained model and predict on the samples in the dataloader. Also prints the accuracy of the model.\n",
    "\n",
    "    Args:\n",
    "        model (RecNeuralNet/RecNeuralNetGRU): Trained RNN/GRU model\n",
    "        dataloader (Dataloader): Dataloader which contains the test data\n",
    "        modelName (string): Name of model to differentiate between different RNN/GRU models (For question 5a and 5b)\n",
    "        device (string): 'cuda' or 'cpu' based on GPU availability\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        value, predictions = torch.max(outputs.data, 1)\n",
    "        correct += (predictions == targets).sum().item()\n",
    "    print(f'Accuracy of {modelName} - {correct/len(dataloader.dataset)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RNN - 0.47445\n"
     ]
    }
   ],
   "source": [
    "predictRNN(model, testLoader, 'RNN', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 5b\n",
    "- Accuracy achieved - 0.49195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecNeuralNetGRU(nn.Module):\n",
    "    def __init__(self, vocabSize, embeddingDim, hiddenDim, embeddings, outputSize):\n",
    "        super(RecNeuralNetGRU, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocabSize, embeddingDim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        self.gru = nn.GRU(embeddingDim, hiddenDim, batch_first=True)\n",
    "        self.fc = nn.Linear(hiddenDim, outputSize)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.embeddings(input)\n",
    "        gruOutput, hidden = self.gru(output)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.002520\n",
      "Epoch: 2 \tTraining Loss: 0.002263\n",
      "Epoch: 3 \tTraining Loss: 0.002204\n",
      "Epoch: 4 \tTraining Loss: 0.002168\n",
      "Epoch: 5 \tTraining Loss: 0.002139\n",
      "Epoch: 6 \tTraining Loss: 0.002120\n",
      "Epoch: 7 \tTraining Loss: 0.002098\n",
      "Epoch: 8 \tTraining Loss: 0.002079\n",
      "Epoch: 9 \tTraining Loss: 0.002064\n",
      "Epoch: 10 \tTraining Loss: 0.002049\n",
      "Epoch: 11 \tTraining Loss: 0.002028\n",
      "Epoch: 12 \tTraining Loss: 0.002020\n",
      "Epoch: 13 \tTraining Loss: 0.002011\n",
      "Epoch: 14 \tTraining Loss: 0.001996\n",
      "Epoch: 15 \tTraining Loss: 0.001985\n"
     ]
    }
   ],
   "source": [
    "############## MODEL CREATION ##############\n",
    "hiddenDim = 20\n",
    "outputDim = 5\n",
    "learningRate = 1e-2\n",
    "numEpochs = 15\n",
    "embeddingDim = 300\n",
    "\n",
    "model = RecNeuralNetGRU(vocab.shape[0], embeddingDim, hiddenDim, amazonWeightsMatrix, outputDim)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "############## MODEL TRAINING ##############\n",
    "for epoch in range(numEpochs):\n",
    "    trainLoss = 0.0\n",
    "    for data, labels in trainLoader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainLoss += loss.item()\n",
    "    trainLoss = trainLoss/len(trainLoader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, trainLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GRU - 0.49195\n"
     ]
    }
   ],
   "source": [
    "# 15 epochs\n",
    "predictRNN(model, testLoader, 'GRU', device)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6012857e81e8cff14b1b6940147b062794364b8096ad9a2e1427430ee5b32590"
  },
  "kernelspec": {
   "display_name": "timesformer-pykernel",
   "language": "python",
   "name": "timesformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
